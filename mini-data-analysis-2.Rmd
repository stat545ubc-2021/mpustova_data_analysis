---
title: "Mini Data Analysis Milestone 2"
output: git_document 
---

Begin by loading your data and the tidyverse package below: 

```{r, message = FALSE}
suppressMessages(library(datateachr)) # <- might contain the data you picked!
suppressMessages(library(tidyverse))
```


# Task 1: Process and summarize your data (15 points)

### 1.1 (2.5 points) 

First, write out the 4 research questions you defined in milestone 1 were. This will guide your work through milestone 2:


<!-------------------------- Start your work below ---------------------------->

After the further exploration of data, some of the research questions should be adjusted.

Specifically, the initial version of Question 1 was: *Have the maximum flow events increased in magnitude or frequency since 1909?*. However, the exploration of the dataset showed that onle one maximum and minimum flow event was recorded per year. Hence, one cannot analyze whether the frequency of such events have increased. Therefore, Question 1 was changed to: *Have the maximum and minimum flow events increased in magnitude since 1909?*. The new question focus on both maximum and minimum flow, which additionally adds complexity to the analysis. 

Another question that have been adjusted is Question 4, which initial version was as follows: *Have the number of data entries that are estimated or have significant gaps during the day (sym=E or sym=B, respectively) decreased over time?* First issue that have been found, is that the question had a typo: symbol A, not B corresponds to the estimatates due to significant gaps during the day. Secondly, it has been found, that most of the symbols present in the dataset was "B", which indicates that the streamflow value was estimated with consideration for the presence of ice in the stream. Hence, it may be valuable to include symbols A, B, and E in the analysis. Hence, the new version of the research question is: *Have the number of data entries that are estimated, have significant gaps during the day, or have been affected by the presence of ice (sym=E, sym=A, and sym=B, respectively) decreased over time?*


Hence, the final version of the research question is as follows:


1. **Have the maximum and minimum flow events increased in magnitude since 1909?**

2. **When did the absolute maximum and minimum magnitude of the flow occur?**

3. **Does the annual distribution of the extreme flow events changed over time? For example, were most of the minimum/maximum that have occurred in past 10 years observed in a similar time than those in 1909-1919 or have the pattern shifted?**

4. **Have the number of data entries that are estimated, have significant gaps during the day, or have been affected by the presence of ice (sym=E, sym=A, and sym=B, respectively) decreased over time?**

<!----------------------------------------------------------------------------->

### 1.2 (10 points)

**Summarizing:**

1. Compute the *range*, *mean*, and *two other summary statistics* of **one numerical variable** across the groups of **one categorical variable** from your data.
2. Compute the number of observations for at least one of your categorical variables. Do not use the function `table()`!
3. Create a categorical variable with 3 or more groups from an existing numerical variable. You can use this new variable in the other tasks! *An example: age in years into "child, teen, adult, senior".*
4. Based on two categorical variables, calculate two summary statistics of your choosing.

**Graphing:**

5. Create a graph out of summarized variables that has at least two geom layers.
6. Create a graph of your choosing, make one of the axes logarithmic, and format the axes labels so that they are "pretty" or easier to read. 
7. Make a graph where it makes sense to customize the alpha transparency.
8. Create 3 histograms out of summarized variables, with each histogram having different sized bins. Pick the "best" one and explain why it is the best.

Make sure it's clear what research question you are doing each operation for!




<!------------------------- Start your work below ----------------------------->

Before starting the analysis, additional libraries should be loaded:

```{r}
suppressMessages(library(lubridate))
```





## Question 1: Have the maximum and minimum flow events increased in magnitude since 1909?

Before starting the analysis, I would like to mention again that the first research question was changed (intitial version was: *Have the maximum flow events increased in magnitude or frequency since 1909?*)

This is due to the face that after further exploring the dataset, it became apparent that only one maximum and one minimum extreme weather event is recorded per each year (as shown in the following code):

```{r}
#The following function computes summary statistics showing the number of entries for each year and each extreme type 

flow_sample_observations <- flow_sample %>%
    select(year, extreme_type, flow) %>%
    group_by(year, extreme_type) %>%
    summarize(n=n())

print(flow_sample_observations)

```


**Because of this, the following analysis will be focused on answering a revised research question - i.e. Have the maximum and minimum flow events increased in magnitude since 1909?**


### Summarizing

To achieve this, I decided to do the following *summarizing tasks* for this research question:
1. **Create a categorical variable with 3 or more groups from an existing numerical variable. You can use this new variable in the other tasks! *An example: age in years into "child, teen, adult, senior".**
2.**Based on two categorical variables, calculate two summary statistics of your choosing:**

Firstly, new categorical data will be created - decade.
Next, based on the categorical variables "decade" and "extreme_type", the range and mean of the flow will be derived using summary statistics.

Looking at the range and mean of the maximum and minimum flow events per each of the decade, I should be able to conclude whether the magnitude of extreme flow events changed over time. 


As have been outlined before, I will firstly create a new categorical variable "decade" which will hold information on which decade the data entries correspond to:

```{r}
#creating new column "decade":
flow_sample_decades <- flow_sample %>%
  mutate(decade = case_when(year<1920 ~ "1909-1919",
                          year<1930 ~ "1920-1929",
                          year<1940 ~ "1930-1939",
                          year<1950 ~ "1940-1949",
                          year<1960 ~ "1950-1959",
                          year<1970 ~ "1960-1969",
                          year<1980 ~ "1970-1979",
                          year<1990 ~ "1980-1989",
                          year<2000 ~ "1990-1999",
                          year<2010 ~ "2000-2009",
                          year<2019 ~ "2010-2018"))

#Checking the result:
head(flow_sample_decades)

```

Since the analysis will be focused on categorical variables "extreme_type" and "decade", and numerical variable "flow", it is important to remove missing values from the column that have any. Newly created column "decade" does not have missing values, because the column years based on which it was created did not have missing values (as was evident in Milestone 1). Hence, missing values have to be removed only for variables "extreme_type" and "flow":

```{r}
#Filtering out missing data from the column "extreme_type" and "flow" and storing the result in a new dataframe called "flow_nomissing"
flow_nomissing <- flow_sample_decades %>%
  filter(!is.na(extreme_type), !is.na(flow))
```


Next, the summary statistics of numerical variable "flow" based on categorical variables "extreme_type" and "decade" shall be calculated.
 
```{r}
#Calculating summary statistics:
flow_summary <- flow_nomissing %>%
  group_by(decade,extreme_type) %>%
  summarize(flow_mean = mean(flow), flow_range = range(flow))

print(flow_summary)

```

This summary stastics helps answering the question on whether the magnitude of extreme flow events have increased over time by providing insight on the mean and range of flow by decade. However, for more effective analysis, this data should be visualized. 


### Graphing

I decided to do the following task for this research question: **Create a graph out of summarized variables that has at least two geom layers.**

By plotting mean and range of flow per decade as two separate geom layer on a single graph, one should be able to see whether there have been changes in the magnitude of flow over time. 

The following code will create a graph showing:
1. the range of minimum and maximum extreme flow events per decades (*first geom layer*), where the two types of extreme flow events will be represented by different colors
2. the mean of the minimum and maximum extreme flow events per decade (*second geom layer*).

Logarithmic scale is utilized to ensure that the range of both minimum and maximum flow events is clearly visible.

```{r}
flow_summary_figure <- flow_summary %>%
  ggplot(aes(decade, flow_range))+
  geom_point(aes(color=extreme_type))+
  geom_point(shape=8,aes(decade, flow_mean))+
  scale_y_log10()+ 
  labs(y="flow")+
  coord_flip()


print(flow_summary_figure)
```

The figure suggest that magnitude (range of values) of maximum flow events have increased since 1909, with the greatest range observed in 2010-2018. As for the magnitude (range) of minimum flow events, however, no significant trend can be observed.
Mean values show fluctuations between the decades for both maximum and minimum flow decades. However, no significant trend (e.g. increase or decrease over time) can be derived.




## Question 2: When did the absolute maximum and minimum magnitude of the flow occur?

### Summary

To approach this question, I will do the following task: **Compute the range, mean, and two other summary statistics of one numerical variable across the groups of one categorical variable from your data.**

Based on the cateogircal variable "extreme type", a summary statistics will be computed includig: 
1. mean
2. range
3. median
4. standard deviation
5. maximum
6. minimum
7. number of observations

This will, firstly, allow to determine absolute maximum and minimum value of flow on record in this dataset, which is a first step to determining when such event have occurred.

Secondly, it will improve our understanding of the data - i.e. is standard deviation large and how does standard deviation of maximum and minimum flow events compare, is the mean and median of each type of extreme events close (are the two distributions symmetrical)?


The following code will compute summary statistics:

```{r}
flow_extreme_summary <- flow_nomissing %>%
  group_by(extreme_type) %>%
  summarize(flow_mean = mean(flow), flow_range = range(flow), flow_median =median(flow), flow_sd = sd(flow), flow_max = max(flow), flow_min = min(flow), n=n())

print(flow_extreme_summary)

```

As can be seen from the table, the absolute minimum value of flow is 3.62, while the absolute maximum value is 466. THe mean and the medium is quite close for both maximum and minimum extreme events, hence the distribution of each of them is likely symmetrical. However, based on standard deviation, the spread of the values for maximum flow events is significantly larger than for minimum flow events.



### Graphing

To visualize data for this research question, the following tasks will be completed: 
**Make a graph where it makes sense to customize the alpha transparency.**
and
**Create a graph out of summarized variables that has at least two geom layers.**


In the following, two graphs will be created: 
1. a graph showing all the data points of "flow" over time. This will allow to determine in which year or decade the absolute minimum and maximum flow have occurred.
2. a graph showing all the data points of "flow" sorted by month. This will allow to determine in which month the absolute maximum and minimum flow have occurred.


#### Graph1

To plot the first graph, the new column should be created with the data of each flow event (combining year, month, and day).

```{r}
#In the following the rows with missing values of month, year, and day will be filtered out, and new column holding date will be created:

flow_date <- flow_nomissing %>%
   filter(!is.na(month), !is.na(year), !is.na(day)) %>%
  mutate(date = mdy(paste(flow_nomissing$month, flow_nomissing$day, flow_nomissing$year, sep=" ")))
```


Next, the graph can be plotted. The logarithmic scale will be used so that the variations in both minimum and maximum flow events are represented. *The transparency will be adjusted due to the fact that some of the points overlap*:

```{r}
flow_date_figure <- flow_date %>%
  ggplot(aes(date, flow))+
  geom_point(aes(color=extreme_type),alpha=0.5, size=2)+
  scale_y_log10()

print(flow_date_figure)


```

Presented graph shows the maximum and minimum extreme flow events that have occurred every year. The minimum and maximum extreme events are shown in different colors.
From the graph, one can derive that the minimum extreme flow occurred around 1932, while the maximum was observed around 2013. 





####Graph2
The second graph will show the flow data sorted by month.


Firstly, missing values will be filtered out for the columns "flow", "extreme_type", and "month" since the analysis are focused on these values:

```{r}
flow_nomissing2 <- flow_sample %>%
  filter(!is.na(extreme_type), !is.na(flow), !is.na(month))
```

Next, the variable "month" will be transformed into a categorical variable:

```{r}

flow_nomissing2$month=as.factor(flow_nomissing2$month)
```

Next, the data will be plotted as jitter plot and boxplot (*The graph will consist of two geom layers*). The logarithmic scale will be used so that the variations in both minimum and maximum flow events are represented. *The transparency will be adjusted due to the fact that some of the points overlap*.

```{r}
flow_month_figure <- flow_nomissing2 %>%
  ggplot(aes(month, flow))+
  geom_boxplot()+
  geom_jitter(width=0.2, alpha=0.2, size=2)+
  scale_y_log10()

print(flow_month_figure)
```

Presented graph shows the values of flow per months. The data encompass all of the years in the dataset. The boxplots show the median, lower and upper quantile, minimum, maximum, and outliers.
As can be seen from the graph, the absolute minimum flow occurred in January, while the absolute maximum occured in June. Moreover, it can be derived that the former is an outlier for that month, while the latter is within the minimum range (upper quntile + 1.5IQR). 



## Question 3: Does the annual distribution of the extreme flow events changed over time? For example, were most of the minimum/maximum that have occurred in past 10 years observed in a similar time than those in 1909-1919 or have the pattern shifted?

### Summary

To make a progress towards answering this question, the following task will be completed: **Compute the number of observations for at least one of your categorical variables.**

Computing a number of observations of maximum and minimum extreme weather events for decades "1909-1919" and "2000-2018" will help to determine whether there is a sufficient number of data points for future analysis comparing these two decades. 

Hence, firstly, a subset will be created with only the decades in question. *flow_nomissing* dataframe will be used as a basis for the subset, as it does not contain missing values for variables "flow" and "extreme_type", which will be the focus of analysis in this research question.

```{r}
#creating a subset which only contains two decades in question and missing data
flow_q3 <- flow_nomissing %>%
  filter(decade == c("1909-1919", "2010-2018"))
```

Computing the number of observations for each type of extreme weather events to see whether they are comparable

```{r}
flow_q3_summary <- flow_q3 %>%
  group_by(decade, extreme_type) %>%
  summarize(n=n())

print(flow_q3_summary)
```

As can be seen from the tibble, the number of observations are relatively low - i.e. 4-6 observations per extreme_type per decade. However, it may be argued that it may still be valuable to further explore the differences between these decades.



### Graphing
To further explore the differences between the extreme flow events observed in 1909-1919 and those in 2010-2018, the following task will be completed: **Create a graph out of summarized variables that has at least two geom layers**

Two graphs will be created - one for minimum and one for maximum extreme_type - showing the timing of extreme weather events in both of the decades. Moreover, the mean of flow and timing for each decade will be plotted to enable comparison.

To achieve this, firstly, a column will be added to the subset with the information on the day of the year in which the extreme flow have occurred: 

```{r}
#The following code filters out missing values of month, year, and day, as well as creates a new column which hold which day of the year an extreme flow event corresponds to:

flow_q3_daynumber <- flow_q3 %>%
   filter(!is.na(month), !is.na(year), !is.na(day)) %>%
   mutate(day_number = yday(mdy(paste(flow_q3$month, flow_q3$day, flow_q3$year, sep=" "))))
print(flow_q3_daynumber)


```

Next, two new dataframes will be created which will show per each decade, what was the mean flow and mean day of the year when extreme_type events occur. One dataframe will hold information for maximum flow events, while another will consist only of minimum flow events. Creating two dataframes (as opposite to one) will enable to plot the maximum and minimum flow events in a separate graphs.


```{r}
#The following code (1) filters out minimum extreme type events, (2) selects columns of interest - i.e. day_number, flow, decade, (3) groups the data by decade, (4) and computes the mean of the day_number and flow per each of the decade:

flow_q3_summary_max <- flow_q3_daynumber %>% 
        filter(extreme_type=="maximum") %>%
        select(day_number, flow, decade) %>%
        group_by(decade) %>%
        summarise(day_number = mean(day_number),
                  flow  = mean(flow))
print(flow_q3_summary_max)

#The following code (1) filters out maximum extreme type events, (2) selects columns of interest - i.e. day_number, flow, decade, (3) groups the data by decade, (4) and computes the mean of the day_number and flow per each of the decade:

flow_q3_summary_min <- flow_q3_daynumber %>% 
        filter(extreme_type=="minimum") %>%
        select(day_number, flow, decade) %>%
        group_by(decade) %>%
        summarise(day_number = mean(day_number),
                  flow  = mean(flow))
print(flow_q3_summary_min)



```


The following code will create a plot which will show on which days of the year maximum flow occured in 1909-1919 versus 2010-2018. Moreover, the mean of flow and day of the year will be plotted to enable comparison.

```{r}
flow_q3_plot_max <-flow_q3_daynumber %>%
  filter(extreme_type == "maximum")%>% #filtering out minimum flow events
  ggplot(aes(day_number, flow))+ 
  geom_point(aes(color=decade))+ #the decades will be color-coded
  geom_point(data=flow_q3_summary_max, size=6, aes(color=decade),shape=8)
  #adding the mean for each decade, ensuring that it is represented with a different shape and that decades are color-coded similarly to the previous layer

print(flow_q3_plot_max)

```

As can be seen from the graph, maximum flow occurred on average about 4 days earlier in 2010-2018 than in 1909-1919. This may imply that there was a change in the pattern over this period of time (e.g. due to changing climate)



The same graph will be created for minimum flow events:
```{r}
flow_q3_plot_min <-flow_q3_daynumber %>%
  filter(extreme_type == "minimum")%>% #filtering out maximum flow
  ggplot(aes(day_number, flow))+
  geom_point(aes(color=decade))+
  geom_point(data=flow_q3_summary_min, size=6, aes(color=decade),shape=8)
print(flow_q3_plot_min)

```

As can be seen from the graph, the timing of the minimum flow have changed significantly. While in 1909-1919 minimum flow was observed ob average on about 125th day of the year, in 2010-2018, it shifted to about 50th day.
However, by looking at the graph, one should note that one of the minimum flow events in 1909-1919 decade occurred in December, while none of the 2010-2018 occurred in December. This might have scewed the results for 1909-1919 decade hence undermining the analysis. 


Finally it's important to note that the number of datapoints for each type of extreme events and each decade is extremely small. To ensure more reliable results, one may consider comparing larger periods of time - e.g. 1909-1960 and 1960-2018.



## Question 4: Have the number of data entries that are estimated or have significant gaps during the day (sym=E or sym=A, respectively) decreased over time?

**NOTE: in original question, it was indicated that symbol for the significant gaps in data during the day is B. However, the correct symbol is A, which have been corrected**

### Summarizing

To address this research question, the following task was chosen: **Compute the number of observations for at least one of your categorical variables. Do not use the function table()!**
Specifically, the number of observations of each of the categories of sym per decade. 
The website of Government of Canada (Water Level and Flow) https://wateroffice.ec.gc.ca/contactus/faq_e.html#Q12 outlines the meaning of each symbols:
* E. The symbol E indicates that there was no measured data available for the day or missing period, and the water level or streamflow value was estimated by an indirect method such as interpolation, extrapolation, comparison with other streams or by correlation with meteorological data.

*A. The symbol A indicates that the daily mean value of water level or streamflow was estimated despite gaps of more than 120 minutes in the data string or missing data not significant enough to warrant the use of the E symbol.

* B. The symbol B indicates that the streamflow value was estimated with consideration for the presence of ice in the stream. Ice conditions alter the open water relationship between water levels and streamflow.

* D. The symbol D indicates that the stream or lake is "dry" or that there is no water at the gauge. This symbol is used for water level data only.

* R. The symbol R indicates that a revision, correction or addition has been made to the historical discharge database after January 1, 1989.

This information is important for future analysis of data. 


Before conducting the summary statistics, the rows with missing symbols will be removed. An assumption is made that if the missing symbol mean lack of any of the outlined above conditions. It is important to note, however, that this might not be the case and there could be other reasons for gaps in data. Hence, large number of missing data for "sym" variable can constitute a limitation.

Filtering out missing values and selecting only the columns of interest (sym and decade)


```{r}
flow_sym <- flow_date %>%
  select(decade,sym) %>% #selecting variables in question
  filter(!is.na(sym)) #removing rows with missing data in "sym" column

#checking the result:
head(flow_sym)
```

Next, summary statistics will be computed showing the number of observations of each category of sym per each of the decades. 


```{r}
flow_sym_summary <- flow_sym %>%
  group_by(decade, sym) %>%
  summarize(n=n())

print(flow_sym_summary)
```

The presented table shows the number of observations of different symbols per decade. As can be derived from the table, some of the symbols have been observed in just a few decades - e.g. sym "E" was only observed once in decade 1920-1929 and once in 1980-1989. Similarly sym "A" was recorded only for decades 1990-1999 and 1980-1989 and have only appeared once in both of these decades. 
The most popular symbol is "B", which appeared in every decade represented in the dataset.



### Graphing
As for the graphing, none of the proposed tasks could be applied for this research question. 

However, the data in question could be effectively represented using a bar chart.


```{r}

#The following code plots a bar chart showing the number of symbols for each of the decades, where different symbols are represented by different colors. 
#The coordinates are flipped for better visual representation (to avoid overlapping text on the x-axis).
#The labels include a short explanation of the meaning of each of the symbol

flow_sym_plot <-flow_sym %>% 
  ggplot(aes(decade))+
  geom_bar(aes(fill=sym))+ 
  coord_flip()+
  scale_fill_discrete(name="Symbol",
                      breaks=c("A", "B", "E"),
                      labels=c("A (significant daily gaps)", "B (ice)",
                               "E(estimates for missing data)"))

#Created bar chart, however, starts at the latest decade and ends at the oldest, which may not be the most effective way to visualize data. 

#The following code will flip the categories, so that the graph starts at the oldest decade. The solution was inspired by the post under the following link: https://community.rstudio.com/t/reverse-order-of-categorical-y-axis-in-ggridges-ggplot2/2273/2

flow_sym_plot2 <- flow_sym_plot + scale_x_discrete(limits = rev(unique(sort(flow_sym$decade)))) #while decades are represented on the y-axis, scale_x_discrete is used, because the coordinates were previously flipped and the decades are still regarded as x-axis.

print(flow_sym_plot2)


```
As can be seen from the graoh, in past two decades, there have not been any data with significant daily gaps (A) or where the data was missing entirely and had to be estimated (E). This may be due to improvement in the recording instruments. However, it is important to note that no A or E symbols appeared between 1930s-1980s. Hence it cannot be conclusively derived whether there have been a significant improvement and no missing data will occur in the future. 

Moreover, as can be seen from the graph, there are fluctuations in the number of "B" symbols which indicate that the streamflow value was estimated with consideration for the presence of ice in the stream. While no specific trend can be derived, it can be assumed that the decrease in the number of "B" symbols in last decade (2010-2018) could be attributed to either lower presence of ice due to climate change or the fact that 2010-2018 category has one less year than other categories. 





<!----------------------------------------------------------------------------->

### 1.3 (2.5 points)

Based on the operations that you've completed, how much closer are you to answering your research questions? Think about what aspects of your research questions remain unclear. Can your research questions be refined, now that you've investigated your data a bit more? Which research questions are yielding interesting results?

<!------------------------- Write your answer here ---------------------------->


<!----------------------------------------------------------------------------->

# Task 2: Tidy your data (12.5 points)

In this task, we will do several exercises to reshape our data. The goal here is to understand how to do this reshaping with the `tidyr` package.

A reminder of the definition of *tidy* data:

- Each row is an **observation**
- Each column is a **variable**
- Each cell is a **value**

*Tidy'ing* data is sometimes necessary because it can simplify computation. Other times it can be nice to organize data so that it can be easier to understand when read manually. 

### 2.1 (2.5 points)

Based on the definition above, can you identify if your data is tidy or untidy? Go through all your columns, or if you have >8 variables, just pick 8, and explain whether the data is untidy or tidy.

<!--------------------------- Start your work below --------------------------->


Since the tidyness of data depend on the research question, I will be tidying/untidying data specifically for question 1 - i.e. *Have the maximum flow events increased in magnitude or frequency since 1909?*

To determine whether the data is tidy, it may be valuable to first look at the dataset:
```{r}
head(flow_sample)
```

To answer this question, the **observation** would be a flow value of the maximum and minimum flow events per each of the years. In the presented dataset, such observations correspond to the rows of the dataset.

The **variables** of interest - i.e. year, extreme_type, and flow - are represented as column of the dataset.

Finally, each of the cell represent the **value** corresponding to these variables - values that can be used to answer the research question.

Therefore, it may be concluded that the dataset is in a tidy form for this research question. 

<!----------------------------------------------------------------------------->

### 2.2 (5 points)

Now, if your data is tidy, untidy it! Then, tidy it back to it's original state.

If your data is untidy, then tidy it! Then, untidy it back to it's original state.

Be sure to explain your reasoning for this task. Show us the "before" and "after".

<!--------------------------- Start your work below --------------------------->

#### Untidying data

In the following section, I will untidy the dataset. Specifically, I will create new columns from the categorical variable "extreme_type", which will contain values of flow. 

It is important to note, however, that there might be problems tidying this data. This is due to the fact that when I will re-create variable "extreme_type", it will consist of the double the amount of rows. For example, my original dataset has a row that shows that in 7th day of 7th months of 1909, there was a maximum extreme type of event recorded with a flow value of 314. After, I will untidy the dataset by putting "maximum" and "minimum" categories as separate variables/columns and tidy it again, I will have one more column created per each observation. For example, I will have another row that shows that in 7th day of 7th months of 1909, there was a maximum extreme type of event recorded with a missing value. This is due to the fact that maximum and minimum extreme type events cannot occur on the same date. THis could be fixed by removing rows where the flow value is missing (NA). However, if the intitial dataset had missing values of flow, they will be removed as well. To avoid that, before untidying my data I will first replace missing values in the column "flow" with the string "missing". When I will be tidying data, this will allow to differentiate between the rows with missing flow value due to the tidying/untidying and due to gaps in the original dataset. Therefore, loss of data will be prevented.

THe following code will create a new variable - flow_sample1 - and substitute all the missing values in column "flow" with the string "missing". New variable was created to leave the original dataset intact.

```{r}
#Creating a copy of the dataset:
flow_sample1 <- flow_sample

#The following code substitutes missing values of flow with "missing":
flow_sample1$flow[is.na(flow_sample1$flow)] <- "missing"

```


Now, the dataset shall be untidied by creating new columns for each type of the extreme type, which will hold the values of flow:

```{r}
flow_sample_untidy <- flow_sample1 %>%
  pivot_wider(id_cols = c(-extreme_type, -flow), 
                names_from = extreme_type,
                values_from = flow)
print(flow_sample_untidy)
```

This dataset is not tidy for research question 1, because **each row does not represent represent each instance of observation** - i.e.  a flow value of the maximum and minimum flow events per each of the years. In the presented dataset, such observations correspond to the rows of the dataset.



#### Tidying data

Next, I will tidy the date by creating the column "extreme_type", which will store information on whether a certain recorded flow belonged to "minimum" or "maximum" category:

```{r}
flow_sample_tidy <- flow_sample_untidy %>%
  pivot_longer(cols = c(-station_id, -year, -month, -day, -sym),
               names_to = 'extreme_type',
               values_to = 'flow')
print(flow_sample_tidy)

```

As have been mentioned before, the obtained dataset has double the number of rows due to the fact that two extreme type events (maximum and minimum) could not have occured on the same day, month, and year. 


To return it to its original ("tidy") version, the missing values of flow that were created due to tidying/untidying shall be removed. Following that I will substitute the values of flow that were missing in the original dataset with logical value. This way no data will be lost.

```{r}

#Removing rows where flow value is missing:
flow_sample_tidy_clean <- flow_sample_tidy %>%
  filter(!is.na(flow))
  

#Substituting missing values from the original dataset back with logical value:
flow_sample_tidy_clean$flow[flow_sample_tidy_clean$flow == "missing"] <- NA


print(flow_sample_tidy_clean)
```

The resulting dataframe is "tidy". The reasons have been explained in 2.1 (I have re-created the initial dataset).



<!----------------------------------------------------------------------------->

### 2.3 (5 points)

Now, you should be more familiar with your data, and also have made progress in answering your research questions. Based on your interest, and your analyses, pick 2 of the 4 research questions to continue your analysis in milestone 3, and explain your decision. 

Try to choose a version of your data that you think will be appropriate to answer these 2 questions in milestone 3. Use between 4 and 8 functions that we've covered so far (i.e. by filtering, cleaning, tidy'ing, dropping irrelvant columns, etc.). 

<!--------------------------- Start your work below --------------------------->






<!----------------------------------------------------------------------------->

*When you are done, knit an `md` file. This is what we will mark! Make sure to open it and check that everything has knitted correctly before submitting your tagged release.*

### Attribution

Thanks to Victor Yuan for mostly putting this together. 